# ğŸ“Š Projeto Final â€” Trilha Engenharia de Dados | Data Girls

Este projeto foi desenvolvido como parte da trilha de **Engenharia de Dados** do programa **Data Girls**, com foco em aplicar os conhecimentos de **ETL**, **tratamento de dados**, **armazenamento em nuvem (AWS S3)** e **conceitos de orquestraÃ§Ã£o de pipeline com simulaÃ§Ã£o de agendamento**.

## ğŸ¯ Objetivo

Analisar dados de **recursos humanos** (employee attrition) e criar um pipeline de dados completo que:

- Limpa e transforma os dados
- Salva os dados processados
- Realiza upload para o **AWS S3**
- **Simula a execuÃ§Ã£o agendada do processo, demonstrando a automaÃ§Ã£o de pipeline.**
## ğŸ—‚ï¸ Estrutura do Projeto
â”œâ”€â”€ ProjetofinalTrilhaEngenhariaDados_DataGirls.ipynb  # Notebook principal do projeto

â”œâ”€â”€ data/

â”‚   â””â”€â”€ WA_Fn-UseC_-HR-Employee-Attrition.csv          # Dataset original

â””â”€â”€ README.md

## ğŸ§° Tecnologias e Ferramentas

- **Python**
- **Pandas** e **NumPy**
- **Google Colab**
- **AWS S3** com `boto3`
- **Ferramentas Conceituais de OrquestraÃ§Ã£o (Airflow)**
## ğŸ”„ Pipeline ETL

Este pipeline segue as etapas de ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga (ETL) para preparar os dados de RH.

### 1. **ExtraÃ§Ã£o**
- **Fonte:** O dataset original foi importado de um arquivo CSV, `WA_Fn-UseC_-HR-Employee-Attrition.csv`, que estÃ¡ armazenado localmente no repositÃ³rio na pasta `data/`.

### 2. **TransformaÃ§Ã£o**
Esta etapa foca na limpeza, padronizaÃ§Ã£o e enriquecimento dos dados para tornÃ¡-los prontos para anÃ¡lise. As principais transformaÃ§Ãµes incluem:

* **RenomeaÃ§Ã£o de Colunas:** Nomes das colunas traduzidos para portuguÃªs para melhor legibilidade e consistÃªncia (ex: 'Age' para 'idade', 'Department' para 'departamento').
* **TraduÃ§Ã£o de Valores CategÃ³ricos:** Valores em colunas como 'Attrition', 'Gender', 'MaritalStatus', 'BusinessTravel', 'EducationField', 'OverTime' e 'JobRole' foram traduzidos para portuguÃªs (ex: 'Yes' para 'Sim', 'Male' para 'Masculino').
* **NormalizaÃ§Ã£o de Escalas:** Colunas como 'Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobSatisfaction', 'PerformanceRating', 'RelationshipSatisfaction' e 'WorkLifeBalance' tiveram seus valores numÃ©ricos mapeados para descriÃ§Ãµes textuais correspondentes (ex: 1: 'Baixa', 4: 'Excelente').
* **CriaÃ§Ã£o de Novas Features:**
    * `faixa_etaria`: Criada a partir da coluna 'idade', categorizando os funcionÃ¡rios em faixas etÃ¡rias especÃ­ficas (ex: '18-25', '26-35').
    * `desligamento_flag`: Coluna binÃ¡ria (0 ou 1) que indica se o funcionÃ¡rio se desligou (1 para 'Sim', 0 para 'NÃ£o').
    * `faixa_salarial`: Criada atravÃ©s da categorizaÃ§Ã£o da 'renda_mensal' em quartis (Q1_Baixa, Q2_Media, Q3_Alta, Q4_Muito_Alta).
* **One-Hot Encoding:** VariÃ¡veis categÃ³ricas restantes foram convertidas em representaÃ§Ãµes numÃ©ricas binÃ¡rias, utilizando `pd.get_dummies()`, para serem compatÃ­veis com modelos de Machine Learning e anÃ¡lises quantitativas.

### 3. **Carga**
- O dataset transformado Ã© salvo em formato **Parquet** por sua eficiÃªncia em armazenamento e leitura, ideal para uso em Data Lakes.
- O arquivo Parquet Ã© entÃ£o carregado para um bucket no **AWS S3**. Para simular o agendamento, o nome do arquivo no S3 inclui a data da execuÃ§Ã£o, permitindo o versionamento dos dados (`funcionarios_transformados_YYYYMMDD.parquet`).

### 4. **AutomaÃ§Ã£o e SimulaÃ§Ã£o de Agendamento**
Para cumprir o requisito de "execuÃ§Ã£o agendada e simulada", o pipeline ETL foi encapsulado em uma funÃ§Ã£o Python Ãºnica (`executar_pipeline_etl`). A automaÃ§Ã£o foi simulada da seguinte forma no Google Colab:

* A funÃ§Ã£o `executar_pipeline_etl` Ã© chamada repetidamente dentro de um loop.
* Cada chamada simula uma execuÃ§Ã£o em um "dia" diferente, passando a data simulada como parÃ¢metro.
* Um pequeno atraso (`time.sleep()`) foi adicionado entre as execuÃ§Ãµes para visualizaÃ§Ã£o, simulando o intervalo de agendamento.
* Como resultado, cada execuÃ§Ã£o gera um novo arquivo Parquet no S3 com a data da simulaÃ§Ã£o no nome (ex: `s3://datagirlsetl1008/projeto_rh/funcionarios_transformados_20250724.parquet`), demonstrando o versionamento e a capacidade de reexecuÃ§Ã£o agendada.

Em um ambiente de produÃ§Ã£o real, esta orquestraÃ§Ã£o seria gerenciada por ferramentas dedicadas como o **Apache Airflow**, que permitiriam agendamento complexo, monitoramento robusto e recuperaÃ§Ã£o de falhas.

## ğŸ“ Dataset

O dataset utilizado Ã© de domÃ­nio pÃºblico e contÃ©m informaÃ§Ãµes como:

- NÃ­vel educacional
- Cargo
- SalÃ¡rio
- SatisfaÃ§Ã£o no trabalho
- EquilÃ­brio entre vida pessoal e profissional
- Status de desligamento

## â˜ï¸ AWS S3

Bucket criado: `datagirlsetl1008`
Arquivos enviados: `projeto_rh/funcionarios_transformados_YYYYMMDD.parquet` (mÃºltiplos arquivos, um para cada execuÃ§Ã£o simulada).
> A conexÃ£o foi feita utilizando `boto3` com credenciais IAM especÃ­ficas para acesso programÃ¡tico.

## ğŸš€ Como Rodar o Projeto

1.  Clone este repositÃ³rio.
2.  Abra e execute o notebook `ProjetofinalTrilhaEngenhariaDados_DataGirls.ipynb` no Google Colab.
3.  Insira suas credenciais AWS quando solicitado.
4.  Execute a cÃ©lula que define a funÃ§Ã£o `executar_pipeline_etl`.
5.  Execute a cÃ©lula que contÃ©m o loop de simulaÃ§Ã£o de agendamento.
6.  Verifique os arquivos Parquet gerados em seu bucket S3 (eles terÃ£o a data no nome, como `funcionarios_transformados_20250724.parquet`).
## ğŸ‘©â€ğŸ’» Autora

**Carolina Cavalcante Lima**
Economista em transiÃ§Ã£o de carreira para TI, com foco em **Engenharia de Dados** e **Cloud Computing**.

